{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3  ('env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "ffb072dfce72f6f080b2d6b8984a56656ee2b807f16892b8b0f111185ea2e04f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1>Question 1</h1>\n",
    "GitHub repo link: https://github.com/piyushmundhra/spellcheck/blob/main/spellcheck.ipynb\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1>Question 2</h1>\n",
    "<br>All alphabet strings: /[a-zA-Z]*/</br>\n",
    "<br>Social Security Numbers: /[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]/</br>\n",
    "<br>Lowercase alphabet strings ending in 'b': /[a-z]*b/</br>\n",
    "<br>Strings containing 'grotto' and 'raven' as words: /(/braven/b){1}(/bgrotto/b){1}/</br>\n",
    "<br>The set of strings with two consecutive repeated words: /[a-zA-Z]*\\b[a-zA-Z]+\\b\\1\\b[a-zA-Z]*</br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1>Question 3</h1>\n",
    "<h3>3.a</h3>\n",
    "<br>Ishmael is mentioned 16 times in the text</br>\n",
    "<h3>3.b</h3>\n",
    "<br>The following are phrases where Ishmael is not preceded by punctuation: 'CALL me Ishmael', ''WHALING VOYAGE BY ONE ISHMAEL', 'It's I Ishmael', 'would be to dive deeper than Ishmael', 'all these to Ishmael', 'my dear Ishmael' </br>\n",
    "<h3>3.c</h3>\n",
    "<br>There are two lowercase words that begin with 'ant': 'ant-hill' and 'ant-hills'</br>\n",
    "<h3>10 most frequent words:</h3>\n",
    "<br>the, of, and, a, to, in, that, I, his, it</br>\n",
    "<h3>10 least frequent words:</h3>\n",
    "<br>seaweed, indestructible, Humpback, Wise, muttering, hump, Says, merman, badger, spraining</br>\n",
    "<h3>Finding the longest word:</h3>\n",
    "<br>The file tok_by_alph.txt contains all the tokens in Moby Dick, sorted alphabetically. The following code cell reads the file, and looks for the longest word and outputs it. I also edited the file and added 'token' and 'frequency' separated by a tab at the top. Since the header is inferred by default, this made it easier to index the desired column.  </br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          token  frequency\n0                 115314.0\n1             !      739.0\n2             \"      323.0\n3             $        3.0\n4             &        3.0\n...         ...        ...\n13855  zontally        1.0\n13856   zoology        1.0\n13857         |        1.0\n13858         ~        2.0\n13859                  NaN\n\n[13860 rows x 2 columns] \n\n\nLongest Word: cannibalistically\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/piyushmundhra/Desktop/cs173/tok_by_alph.txt\", sep='\\t', lineterminator='\\r', header='infer')\n",
    "df['token'] = df['token'].str.replace('\\\\n','')\n",
    "print(df, '\\n\\n')\n",
    "print('Longest Word:', max(df['token'], key=len))"
   ]
  },
  {
   "source": [
    "<h1>Question 4</h1>\n",
    "<br>\n",
    "    <img src=\"img1.png\" width=400 />\n",
    "    <img src=\"img2.png\" width=400 />\n",
    "</br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1> Question 5 </h1>\n",
    "<br> NLTK is a library that makes it significantly easier to preprocess text and apply language processing algorithms </br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1>Question 6</h1>\n",
    "<h3>6.a</h3>\n",
    "<br>The following are NLTK data installation commands: 'python -m nltk.downloader all', 'sudo python -m nltk.downloader -d /usr/local/share/nltk_data all'</br>\n",
    "<h3>6.b</h3>\n",
    "<br>The following are all the nltk stop words: </br>\n",
    "<br>\n",
    "i\n",
    "me\n",
    "my\n",
    "myself\n",
    "we\n",
    "our\n",
    "ours\n",
    "ourselves\n",
    "you\n",
    "you're\n",
    "you've\n",
    "you'll\n",
    "you'd\n",
    "your\n",
    "yours\n",
    "yourself\n",
    "yourselves\n",
    "he\n",
    "him\n",
    "his\n",
    "himself\n",
    "she\n",
    "she's\n",
    "her\n",
    "hers\n",
    "herself\n",
    "it\n",
    "it's\n",
    "its\n",
    "itself\n",
    "they\n",
    "them\n",
    "their\n",
    "theirs\n",
    "themselves\n",
    "what\n",
    "which\n",
    "who\n",
    "whom\n",
    "this\n",
    "that\n",
    "that'll\n",
    "these\n",
    "those\n",
    "am\n",
    "is\n",
    "are\n",
    "was\n",
    "were\n",
    "be\n",
    "been\n",
    "being\n",
    "have\n",
    "has\n",
    "had\n",
    "having\n",
    "do\n",
    "does\n",
    "did\n",
    "doing\n",
    "a\n",
    "an\n",
    "the\n",
    "and\n",
    "but\n",
    "if\n",
    "or\n",
    "because\n",
    "as\n",
    "until\n",
    "while\n",
    "of\n",
    "at\n",
    "by\n",
    "for\n",
    "with\n",
    "about\n",
    "against\n",
    "between\n",
    "into\n",
    "through\n",
    "during\n",
    "before\n",
    "after\n",
    "above\n",
    "below\n",
    "to\n",
    "from\n",
    "up\n",
    "down\n",
    "in\n",
    "out\n",
    "on\n",
    "off\n",
    "over\n",
    "under\n",
    "again\n",
    "further\n",
    "then\n",
    "once\n",
    "here\n",
    "there\n",
    "when\n",
    "where\n",
    "why\n",
    "how\n",
    "all\n",
    "any\n",
    "both\n",
    "each\n",
    "few\n",
    "more\n",
    "most\n",
    "other\n",
    "some\n",
    "such\n",
    "no\n",
    "nor\n",
    "not\n",
    "only\n",
    "own\n",
    "same\n",
    "so\n",
    "than\n",
    "too\n",
    "very\n",
    "s\n",
    "t\n",
    "can\n",
    "will\n",
    "just\n",
    "don\n",
    "don't\n",
    "should\n",
    "should've\n",
    "now\n",
    "d\n",
    "ll\n",
    "m\n",
    "o\n",
    "re\n",
    "ve\n",
    "y\n",
    "ain\n",
    "aren\n",
    "aren't\n",
    "couldn\n",
    "couldn't\n",
    "didn\n",
    "didn't\n",
    "doesn\n",
    "doesn't\n",
    "hadn\n",
    "hadn't\n",
    "hasn\n",
    "hasn't\n",
    "haven\n",
    "haven't\n",
    "isn\n",
    "isn't\n",
    "ma\n",
    "mightn\n",
    "mightn't\n",
    "mustn\n",
    "mustn't\n",
    "needn\n",
    "needn't\n",
    "shan\n",
    "shan't\n",
    "shouldn\n",
    "shouldn't\n",
    "wasn\n",
    "wasn't\n",
    "weren\n",
    "weren't\n",
    "won\n",
    "won't\n",
    "wouldn\n",
    "wouldn't\n",
    "</br>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3>6.c</h3>\n",
    "<br>The downloaded files went into /Users/piyushmunhdra/nltk_data </br>\n",
    "<h3>6.d</h3>\n",
    "<br>I only downloaded the stopwords and Brown corpora so far</br>\n",
    "<h3>6.e</h3>\n",
    "<br>The raw text files seem to have annotations after every word indicating the part of speech the word belongs to. This annotation is preceded by a '/'. Each line looks like it represents a sentence, with the overall text appearing to be an article. I am guessing that each file represents a different web page or text origin, and the cat.txt file shows the name of each file and its category (news, lore, letter, or hobby) The brown dataset was created in 1961 and had major additions from a chat room in 2006</br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1>Question 7</h1>\n",
    "<h3>7.a</h3>\n",
    "<br>I didn't have to download anything extra. I think I downloaded some of the tools before getting to this question because some of my classmates mentioned NLTK tools on the slack channel while I was working on problem 1. I decided to use my own tokenizer but I forgot what I downloaded to use the NLTK tokenizer earlier. The code below is the tokenizing example </br>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), ('...', ':'), ('Arthur', 'NNP'), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning... Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "source": [
    "<h3>7.b</h3>\n",
    "<br>It looks to me that tagged is creating a tuple for every word, where the tuple contains a word and its part of speech. It looks like the entities are groups of words that are related in meaning. To be clear, each subtree in the entity represents a clause within the sentence. Also, I did get the t.draw() to work. Based on the '>>>' markings in the example, it seems to be done within a python notebook since they are getting output without using the print function. So I moved all my code from a .py file to a .ipynb file and it created a new window with the image for me. To make it work on Google Colab, I just took my entire folder (along with my local virtual environment) and plopped it in a new colab folder. Then I ran it and it worked in the same manner\n",
    "</br>\n",
    "<img src=\"img3.png\" width=600 />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1>Question 8</h1>\n",
    "The code below is stolen from you to figure out how many lines there are. I also used built in python functions to print the type of 'data', which is a list. The '/n' is an escape character followed by an 'n', which together represent a newline. Lambda is a function that acts upon a Python data type. It takes in multiple arguments ni a single expression, the return type and overall function is quite vast."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentences: 22924\n<class 'list'>\nWords: 0\n19\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "f = open(\"/Users/piyushmundhra/Desktop/cs173/melville-moby_dick.txt\", 'r')\n",
    "data = f.readlines()\n",
    "print(\"Sentences:\", len(data))\n",
    "print(type(data))\n",
    "print(\"Words:\", len(f.read().split()))\n",
    "\n",
    "# lambda is simply a function that can take multiple arguments in a single expression\n",
    "newdata = list(map(str.strip, data))\n",
    "newdata = [x.strip() for x in data]\n",
    "corpus = \"\\n\".join(data)\n",
    "\n",
    "print(len(re.findall(r\"\\bIshmael\\b\", corpus)))"
   ]
  },
  {
   "source": [
    "<h1>Question 9</h1>\n",
    "\n",
    "<br>We found all the Ishmael's in the code above! There are 19 of them. The /b indicates a word break in a regular expression. Basically it means that Ishmael should be its own word, not connected to or inside other characters.</br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1>Question 10</h1>\n",
    "<h4>Some of module contents highlighted on the python docs include:</h4>\n",
    "<br>re.compile(): this lets you encapsulate a regular expression into an object/variable so you don't have to keep retyping it if you need to use it again</br>\n",
    "<br>re.ASCII: it makes some of the shorthand regular expression matchers only match ASCII instead of Unicode</br>\n",
    "<br>re.match(): it will find the characters at the beginning of a string that match a specified pattern</br>\n",
    "<br>re.search(): it will look for the first location that a regular expression pattern is found in a string</br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}