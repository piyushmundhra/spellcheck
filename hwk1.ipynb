{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3  ('env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "ffb072dfce72f6f080b2d6b8984a56656ee2b807f16892b8b0f111185ea2e04f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1>Question 2</h1>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1>Question 3</h1>\n",
    "<h3>3.a</h3>\n",
    "<br>Ishmael is mentioned 16 times in the text</br>\n",
    "<h3>3.b</h3>\n",
    "<br>The following are phrases where Ishmael is not preceded by punctuation: 'CALL me Ishmael', ''WHALING VOYAGE BY ONE ISHMAEL', 'It's I Ishmael', 'would be to dive deeper than Ishmael', 'all these to Ishmael', 'my dear Ishmael' </br>\n",
    "<h3>3.c</h3>\n",
    "<br>There are two lowercase words that begin with 'ant': 'ant-hill' and 'ant-hills'</br>\n",
    "<h3>10 most frequent words:</h3>\n",
    "<br>the, of, and, a, to, in, that, I, his, it</br>\n",
    "<h3>10 least frequent words:</h3>\n",
    "<br>seaweed, indestructible, Humpback, Wise, muttering, hump, Says, merman, badger, spraining</br>\n",
    "<h3>Finding the longest word:</h3>\n",
    "<br>The file tok_by_alph.txt contains all the tokens in Moby Dick, sorted alphabetically. The following code cell reads the file, and looks for the longest word and outputs it. I also edited the file and added 'token' and 'frequency' separated by a tab at the top. Since the header is inferred by default, this made it easier to index the desired column.  </br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          token  frequency\n0                 115314.0\n1             !      739.0\n2             \"      323.0\n3             $        3.0\n4             &        3.0\n...         ...        ...\n13855  zontally        1.0\n13856   zoology        1.0\n13857         |        1.0\n13858         ~        2.0\n13859                  NaN\n\n[13860 rows x 2 columns] \n\n\nLongest Word: cannibalistically\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/piyushmundhra/Desktop/cs173/tok_by_alph.txt\", sep='\\t', lineterminator='\\r', header='infer')\n",
    "df['token'] = df['token'].str.replace('\\\\n','')\n",
    "print(df, '\\n\\n')\n",
    "print('Longest Word:', max(df['token'], key=len))"
   ]
  },
  {
   "source": [
    "<h1>Question 4</h1>\n",
    "<br>\n",
    "    <img src=\"img1.png\" width=400 />\n",
    "    <img src=\"img2.png\" width=400 />\n",
    "</br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1> Question 5 </h1>\n",
    "<br> NLTK is a library that makes it significantly easier to preprocess text and apply language processing algorithms </br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h1>Question 6</h1>\n",
    "<h3>6.a</h3>\n",
    "<br>The following are NLTK data installation commands: 'python -m nltk.downloader all', 'sudo python -m nltk.downloader -d /usr/local/share/nltk_data all'</br>\n",
    "<h3>6.b</h3>\n",
    "<br>The following are all the nltk stop words: </br>\n",
    "<br>\n",
    "i\n",
    "me\n",
    "my\n",
    "myself\n",
    "we\n",
    "our\n",
    "ours\n",
    "ourselves\n",
    "you\n",
    "you're\n",
    "you've\n",
    "you'll\n",
    "you'd\n",
    "your\n",
    "yours\n",
    "yourself\n",
    "yourselves\n",
    "he\n",
    "him\n",
    "his\n",
    "himself\n",
    "she\n",
    "she's\n",
    "her\n",
    "hers\n",
    "herself\n",
    "it\n",
    "it's\n",
    "its\n",
    "itself\n",
    "they\n",
    "them\n",
    "their\n",
    "theirs\n",
    "themselves\n",
    "what\n",
    "which\n",
    "who\n",
    "whom\n",
    "this\n",
    "that\n",
    "that'll\n",
    "these\n",
    "those\n",
    "am\n",
    "is\n",
    "are\n",
    "was\n",
    "were\n",
    "be\n",
    "been\n",
    "being\n",
    "have\n",
    "has\n",
    "had\n",
    "having\n",
    "do\n",
    "does\n",
    "did\n",
    "doing\n",
    "a\n",
    "an\n",
    "the\n",
    "and\n",
    "but\n",
    "if\n",
    "or\n",
    "because\n",
    "as\n",
    "until\n",
    "while\n",
    "of\n",
    "at\n",
    "by\n",
    "for\n",
    "with\n",
    "about\n",
    "against\n",
    "between\n",
    "into\n",
    "through\n",
    "during\n",
    "before\n",
    "after\n",
    "above\n",
    "below\n",
    "to\n",
    "from\n",
    "up\n",
    "down\n",
    "in\n",
    "out\n",
    "on\n",
    "off\n",
    "over\n",
    "under\n",
    "again\n",
    "further\n",
    "then\n",
    "once\n",
    "here\n",
    "there\n",
    "when\n",
    "where\n",
    "why\n",
    "how\n",
    "all\n",
    "any\n",
    "both\n",
    "each\n",
    "few\n",
    "more\n",
    "most\n",
    "other\n",
    "some\n",
    "such\n",
    "no\n",
    "nor\n",
    "not\n",
    "only\n",
    "own\n",
    "same\n",
    "so\n",
    "than\n",
    "too\n",
    "very\n",
    "s\n",
    "t\n",
    "can\n",
    "will\n",
    "just\n",
    "don\n",
    "don't\n",
    "should\n",
    "should've\n",
    "now\n",
    "d\n",
    "ll\n",
    "m\n",
    "o\n",
    "re\n",
    "ve\n",
    "y\n",
    "ain\n",
    "aren\n",
    "aren't\n",
    "couldn\n",
    "couldn't\n",
    "didn\n",
    "didn't\n",
    "doesn\n",
    "doesn't\n",
    "hadn\n",
    "hadn't\n",
    "hasn\n",
    "hasn't\n",
    "haven\n",
    "haven't\n",
    "isn\n",
    "isn't\n",
    "ma\n",
    "mightn\n",
    "mightn't\n",
    "mustn\n",
    "mustn't\n",
    "needn\n",
    "needn't\n",
    "shan\n",
    "shan't\n",
    "shouldn\n",
    "shouldn't\n",
    "wasn\n",
    "wasn't\n",
    "weren\n",
    "weren't\n",
    "won\n",
    "won't\n",
    "wouldn\n",
    "wouldn't\n",
    "</br>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3>6.c</h3>\n",
    "<br>The downloaded files went into /Users/piyushmunhdra/nltk_data </br>\n",
    "<h3>6.d</h3>\n",
    "<br>I only downloaded the stopwords and Brown corpora so far</br>\n",
    "<h3>6.e</h3>\n",
    "<br></br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}